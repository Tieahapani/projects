# -*- coding: utf-8 -*-
"""Untitled41.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZfbnSg008kf2oPWin67IlBkKp8Qyb5UC
"""

from google.colab import files
uploaded = files.upload()

# Commented out IPython magic to ensure Python compatibility.
from sklearn.cluster import KMeans
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt
# %matplotlib inline

df = pd.read_csv('income.csv')
df

plt.scatter(df['Age'], df['Income($)'])

km = KMeans(n_clusters = 3)
km

y_predicted = km.fit_predict(df[['Age','Income($)']])
y_predicted
df['cluster'] = y_predicted

df1 = df[df.cluster==0]
df2 = df[df.cluster==1]
df3 = df[df.cluster==2]

plt.scatter(df1.Age, df1['Income($)'], color = 'green')
plt.scatter(df2.Age, df2['Income($)'], color = 'blue')
plt.scatter(df3.Age, df3['Income($)'], color = 'black')

plt.xlabel('Age')
plt.ylabel('Income ($)')
plt.legend()

scaler = MinMaxScaler()
scaler.fit(df[['Income ($)']])
df['Income ($)'] = scaler.transform(df['Income ($)'])

scaler = MinMaxScaler()
# Replacing 'Income ($)' with 'Income($)' to match the actual column name in the DataFrame.
scaler.fit(df[['Income($)']])
df['Income($)'] = scaler.transform(df[['Income($)']])
df

scaler.fit(df.Age)
df.Age = scaler.transform(df.Age)
df

scaler.fit(df[['Age']]) # Pass 'Age' as a DataFrame instead of a Series
df.Age = scaler.transform(df[['Age']]) # Pass 'Age' as a DataFrame to transform as well

df

km = KMeans(n_clusters = 3)
y_predicted = km.fit_predict(df[['Age', 'Income($)']])
y_predicted

df['cluster'] = y_predicted
df

df1 = df[df.cluster ==0]
df2 = df[df.cluster ==1]
df3 = df[df.cluster ==2]
plt.scatter(df1.Age, df1['Income($)'], color = 'green')
plt.scatter(df2.Age, df2['Income($)'], color = 'blue')
plt.scatter(df3.Age, df3['Income($)'], color = 'green')
plt.legend()

k_rng = range(1,10)
sse = []
#sum fo square error
for k in k_rng:
  km = KMeans(n_clusters = k)
  km.fit(df[['Age','Income($)']])
  sse.append(km.inertia_)
  sse

sse

plt.xlabel('k')
plt.ylabel('Sum of squared error')
plt.plot(k_rng, sse)

from sklearn import iris

# Commented out IPython magic to ensure Python compatibility.
from sklearn.datasets import load_iris
iris = load_iris()
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt
from sklearn.cluster import KMeans

# %matplotlib inline

iris = load_iris()

df = pd.DataFrame(iris.data, columns = iris.feature_names)
df

plt.scatter(df['petal length (cm)'], df['petal width (cm)'])
plt

km = KMeans(n_clusters = 2)
km

scaler = MinMaxScaler()
df[['petal length (cm)', 'petal width (cm)']] = scaler.fit_transform(df[['petal length (cm)', 'petal width (cm)']])

df

km = KMeans(n_clusters = 2)
y_predicted = km.fit_predict(df[['petal length (cm)', 'petal width (cm)']])
y_predicted

df['cluster'] = y_predicted
df

df1 = df[df.cluster ==0]
df2 = df[df.cluster ==1]

plt.scatter(df1['petal length (cm)'], df1['petal width (cm)'], color = 'green')
plt.scatter(df2['petal length (cm)'], df2['petal width (cm)'], color = 'blue')

plt.legend()

k_rng = range(1,10)
sse = []
#sum fo square error
for k in k_rng:
  km = KMeans(n_clusters = k)
  km.fit(df[['petal length (cm)','petal width (cm)']])
  sse.append(km.inertia_)
  sse

sse

plt.xlabel('k')
plt.ylabel('Sum of squared error')
plt.plot(k_rng, sse)

from sklearn.datasets import load_iris
from sklearn.cluster import KMeans
from sklearn.preprocessing import MinMaxScaler
from matplotlib import pyplot as plt
import pandas as pd

# Load the iris dataset
iris = load_iris()
df = pd.DataFrame(iris.data, columns=iris.feature_names)

# Scale the data (optional but recommended for KMeans)
scaler = MinMaxScaler()
df[['petal length (cm)', 'petal width (cm)']] = scaler.fit_transform(df[['petal length (cm)', 'petal width (cm)']])

# Visualize clusters for k=2 and k=3
for k in [2, 3]:
    km = KMeans(n_clusters=k)
    y_predicted = km.fit_predict(df[['petal length (cm)', 'petal width (cm)']])
    df['cluster'] = y_predicted

    df1 = df[df.cluster == 0]
    df2 = df[df.cluster == 1]
    if k == 3:
        df3 = df[df.cluster == 2]

    plt.figure(figsize=(8, 6))
    plt.scatter(df1['petal length (cm)'], df1['petal width (cm)'], color='green', label='Cluster 0')
    plt.scatter(df2['petal length (cm)'], df2['petal width (cm)'], color='blue', label='Cluster 1')
    if k == 3:
        plt.scatter(df3['petal length (cm)'], df3['petal width (cm)'], color='red', label='Cluster 2')  # Changed color for clarity
    plt.scatter(km.cluster_centers_[:, 0], km.cluster_centers_[:, 1], color='purple', marker='*', s=200, label='Centroids')  # Plot centroids
    plt.xlabel('Petal Length (cm)')
    plt.ylabel('Petal Width (cm)')
    plt.title(f'K-Means Clustering with k={k}')
    plt.legend()
    plt.show()

# Elbow method to help determine optimal k
k_rng = range(1, 10)
sse = []
for k in k_rng:
    km = KMeans(n_clusters=k)
    km.fit(df[['petal length (cm)', 'petal width (cm)']])
    sse.append(km.inertia_)
    sse

plt.figure(figsize=(8, 6))
plt.xlabel('k')
plt.ylabel('Sum of Squared Error')
plt.plot(k_rng, sse)
plt.title('Elbow Method for Optimal k')
plt.show()

sse

from google.colab import files
uploaded = files.upload()

import pandas as pd
df = pd.read_csv('train.csv')
df

df.drop(['PassengerId', 'Name','SibSp','Parch','Ticket','Cabin','Embarked'], axis = 'columns', inplace = True)
df

target = df.Survived
inputs = df.drop('Survived', axis = 'columns')

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size = 0.2)

from sklearn.naive_bayes import GaussianNB
model = GaussianNB()
model.fit(X_train, y_train)

from google.colab import files
uploaded = files.upload()

import pandas as pd
df = pd.read_csv('spam.csv')
df

import pandas as pd

# Try reading with 'latin-1' encoding
df = pd.read_csv('spam.csv', encoding='latin-1')
# If 'latin-1' doesn't work, try other encodings like 'cp1252', 'iso-8859-1', etc.
# You might need to experiment with different encodings to find the correct one.

df

from google.colab import files
uploaded = files.upload()

import pandas as pd
df = pd.read_csv('cleaned_spam_dataset.csv')
df

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

df = pd.read_csv('cleaned_spam_dataset.csv')

X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label'], test_size = 0.2, random_state = 42)

vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 5000)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

model = MultinomialNB()
model.fit(X_train_tfidf, y_train)
y_pred = model.predict(X_test_tfidf)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

model.predict(y_test)

y_pred = model.predict(X_test_tfidf)  # Use X_test_tfidf instead of y_test
print(y_pred) # Print predicted labels

print(df)

emails = {
    'Hey mohan, can we get together to watch football game tomorrow?',
    'FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv'

}
emails_count = vectorizer.transform(emails)
model.predict(emails_count)

X_test_count = vectorizer.transform(X_test)
model.score(X_test_count, y_test)

model.predict(X_test_count)

from sklearn.datasets import load_wine

df = load_wine()
print(df)

from sklearn.datasets import load_wine
import pandas as pd
wine_data = load_wine()
print(wine_data)
df = pd.DataFrame(wine_data.data, columns = wine_data.feature_names)
df['target'] = wine_data.target
print(df)

from sklearn.datasets import load_wine
import pandas as pd

wine_data = load_wine()  # Load the dataset

# Convert to DataFrame
df = pd.DataFrame(wine_data.data, columns=wine_data.feature_names)
df['target'] = wine_data.target  # Add the target column

print(df)  # Print the first few rows

df.feature_names

wine_data.feature_names

wine_target.target_names

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(wine_data, wine_target, test_size = 0.2, random_state = 45)

from sklearn.model_selection import train_test_split

# Assign wine_data.target to wine_target
wine_target = wine_data.target

# Now use wine_target in train_test_split
X_train, X_test, y_train, y_test = train_test_split(wine_data.data, wine_target, test_size = 0.2, random_state = 45)

from sklearn.naive_bayes import GaussianNB, MultinomialNB
model = GaussianNB()
model.fit(X_train, y_train)

model.score(X_test, y_test)

model.predict([[1050.0]])

model = MulitnomialNB()
model.fit(X_train, y_train)
model.score(X_test, y_test)

model = MultinomialNB() # Corrected the typo from MulitnomialNB to MultinomialNB
model.fit(X_train, y_train)
model.score(X_test, y_test)



input_data = wine_data.data[1].reshape(1,-1)
prediction = model.predict(input_data)
print(prediction)

